[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
# This location can be remote as well to mount logs to S3 or GCS
base_log_folder = /opt/airflow/logs

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = CeleryExecutor

# The SQLAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The URL to the Celery broker
broker_url = kafka://kafka:9092

# The URL to the Celery results backend
result_backend = db+postgresql://airflow:airflow@postgres/airflow

[webserver]
# The base url of your airflow installation
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

[scheduler]
# Task instances are picked up by the scheduler every scheduler_heartbeat_sec seconds.
scheduler_heartbeat_sec = 5

# The number of task instances allowed to run concurrently by the scheduler
max_active_tasks_per_dag = 16

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = mailhog
smtp_starttls = True
smtp_ssl = False
smtp_user = 
smtp_password = 
smtp_port = 1025
smtp_mail_from = airflow@example.com